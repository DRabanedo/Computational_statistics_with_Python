{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# <p style=\"text-align: center;\">Computational Statistics with Python</p>\n",
    "\n",
    "\n",
    "## <p style=\"text-align: center;\">Topics 6-7: Monte Carlo integration and approximation - The EM algorithm\n",
    "</p>\n",
    "\n",
    "## <p style=\"text-align: center;\">Expected lecture time: 2 hours</p>\n",
    "\n",
    "\n",
    "<p style=\"text-align: center;\">Giancarlo Manzi</p>\n",
    "<p style=\"text-align: center;\">Department of Economics, Management and Quantitative Methods</p>\n",
    "<p style=\"text-align: center;\">University of Milan, Milan, Italy</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Monte Carlo numerical algorithms\n",
    "\n",
    "\n",
    "**Numerical integration**\n",
    "\n",
    "\n",
    "- What is Monte Carlo integration?\n",
    "\n",
    "\n",
    "- Monte Carlo integration is a technique for numerical integration using random numbers. \n",
    "\n",
    "\n",
    "- It is a particular Monte Carlo method that computes definite integrals. \n",
    "\n",
    "\n",
    "- One can use numerical approximation like the trapezium rule or the Simpson rule using quadratic polynomials $g(x)$: $\\int^{1}_{0} f(x)dx\\approx \\int^{1}_{0} g(x)dx$. \n",
    "\n",
    "\n",
    "<img src=\"MCInt1.png\" width=\"400\" height=\"800\">\n",
    "\n",
    "\n",
    "- Python has many numerical integration functions: \n",
    "    - The scipy.integrate sub-package provides several integration techniques including an ordinary differential equation integrator. An overview of the module is provided by the help command\n",
    "    - quad          -- General purpose integration.\n",
    "    - dblquad       -- General purpose double integration.\n",
    "    - tplquad       -- General purpose triple integration.\n",
    "    - fixed_quad    -- Integrate func(x) using Gaussian quadrature of order n.\n",
    "    - quadrature    -- Integrate with given tolerance using Gaussian quadrature.\n",
    "    - romberg       -- Integrate func using Romberg integration.\n",
    "    - trapz         -- Use trapezoidal rule to compute integral from samples.\n",
    "    - cumtrapz      -- Use trapezoidal rule to cumulatively compute integral.\n",
    "    - simps         -- Use Simpson's rule to compute integral from samples.\n",
    "    - romb          -- Use Romberg Integration to compute integral from\n",
    "                    (2**k + 1) evenly-spaced samples.\n",
    "\n",
    "\n",
    "- See https://docs.scipy.org/doc/scipy/reference/tutorial/integrate.html for details.\n",
    "\n",
    "\n",
    "\n",
    "- These MC methods have some advantages: \n",
    "    - They converges fast for *smooth* integrands. \n",
    "    - Work well for low dimensions. \n",
    "    - Are deterministic. \n",
    "\n",
    "\n",
    "- ...But also have some disadvantages: \n",
    "    - Not rapid convergence for discontinuities. \n",
    "    - Sometimes they cannot deal with infinite bounds. \n",
    "    - Can give untrustworthy results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Monte Carlo numerical algorithms\n",
    "\n",
    "\n",
    "**Numerical integration**\n",
    "\n",
    "\n",
    "- Instead of using numerical approximation, one can *average* : $\\int^{1}_{0}f(x)dx \\approx E[f(x)]$(!!) \n",
    "\n",
    "\n",
    "- One can simply use the following approximation where $n$ values from r.v. $X_i$ are drawn independently from density $f(x)$: $\\int^{1}_{0}f(x)dx \\approx \\frac{1}{n}\\sum^{n}_{i=1}f(x_i)$. \n",
    "\n",
    "\n",
    "<img src=\"MCInt2.png\" width=\"400\" height=\"800\">\n",
    "\n",
    "\n",
    "<img src=\"MCInt3.png\" width=\"400\" height=\"800\">\n",
    "\n",
    "\n",
    "\n",
    "- Consider the value $\\theta$ of the simple integral: $\\theta=\\int^{1}_{0}f(x)dx$.\n",
    "\n",
    "\n",
    "- The definition of the expectation of a function on a random variable $X$ in the $(0,1)$ support is: $E[f(X)]=\\int^{1}_{0}f(x)p(x)dx$, where $p(X)$ is the density of $X$. \n",
    "\n",
    "\n",
    "- In particular, if $X$ is uniformly distributed in $(0,1)$, then we can write: $E[f(X)]=\\int^{1}_{0}f(x)dx=\\theta$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Monte Carlo numerical algorithms\n",
    "\n",
    "\n",
    "**Numerical integration**\n",
    "\n",
    "\n",
    "-  In domains other than $(0,1)$ we have, for example in $(a,b)$: $\\int^{b}_{a}f(x)dx \\approx \\frac{b-a}{n}\\sum^{n}_{i=1}f(x_i)$. \n",
    "\n",
    "\n",
    "<img src=\"MCInt4.png\" width=\"400\" height=\"800\">\n",
    "\n",
    "\n",
    "- We can also use the *classical (crude) Monte Carlo integration*. How? \n",
    "\n",
    "\n",
    "- Suppose $(\\xi_1, \\xi_2, \\dots , \\xi_n)$ are independent random variables uniformly distributed. \n",
    "\n",
    "\n",
    "- Then $f_i=f(\\xi_i)$ are random variates with expected values $\\theta$, and:\n",
    "\n",
    "$$\n",
    "\\bar{f}=\\frac{1}{n}\\sum^{n}_{i=1}f_i\n",
    "$$\n",
    "\n",
    "can be considered an \"unbiased estimator\" of $\\theta$ with variance:\n",
    "\n",
    "$$\n",
    "E[\\bar{f}-\\theta]^2=\\frac{1}{n-1}\\int^{1}_{0}[f(x)-\\theta]^2 dx.\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Monte Carlo numerical algorithms\n",
    "\n",
    "\n",
    "**Numerical integration**: silly example\n",
    "\n",
    "\n",
    "- Suppose we want to solve (answer: $\\frac{\\pi}{4}$):\n",
    "\n",
    "$$\n",
    "\\int_0^1\\sqrt{1-x^2}dx \n",
    "$$\n",
    "which is the area of a unit-radius quarter-circle):\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"MCInt5.png\" width=\"200\" height=\"800\">\n",
    "\n",
    "\n",
    "- This is equivalent to\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "&\\int_0^1\\sqrt{1-x^2}dx =\\int_0^1\\sqrt{1-x^2}\\cdot1dx=\\int_0^1\\sqrt{1-x^2}p(x)dx\\\\\n",
    "&=E[\\sqrt{1-X^2}]\n",
    "\\text{[where } X \\text{ has density } p(\\cdot)\\sim U(0,1)\\text{]}=E[f(X)],\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "where $f(x)=\\sqrt{1-x^2}$.  The problem is equivalent to evaluating:\n",
    "\n",
    "$$\n",
    "\\theta=E[f(X)],\\;X\\sim U(0,1),\\;f(x)=\\sqrt{1-x^2}.\n",
    "$$ \n",
    "\n",
    "- We can generate $(X_1,X_2,\\ldots,X_n)\\sim U(0,1)$, then estimate $\\theta$ by:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}=\\frac{1}{n}\\sum_{i=1}^n f(X_i). \n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7837464705688125\n",
      "0.7853981633974483\n"
     ]
    }
   ],
   "source": [
    "# Above example in Python\n",
    "import numpy as np\n",
    "import math\n",
    "n = 100000\n",
    "xi = np.random.uniform(0,1,n)\n",
    "theta = (1/n)*np.sum((1-xi**2)**0.5)\n",
    "print(theta)\n",
    "print(math.pi/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Monte Carlo numerical algorithms\n",
    "\n",
    "\n",
    "**MC integration: generalization**\n",
    "\n",
    "\n",
    "- The example before can be generalized when:\n",
    "\n",
    "$$\n",
    "\\theta=E[f(X)],\n",
    "$$\n",
    "\n",
    "where $X$ has a density $p$ not necessarily uniform. \n",
    "\n",
    "\n",
    "- For example, let's take a r.v. $X$ distributed as a standard Cauchy with pdf $p(x;0,1)=\\frac{1}{\\pi(1+x^2)}$ and parameters $0$ and $1$. \n",
    "\n",
    "\n",
    "- We can write, with $f(X)=I_{X>2}(X)$:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "E[f(X)] &=E[I_{X>2}(X)]=\\int_{-\\infty}^\\infty I_{X>2}(X)\\frac{1}{\\pi(1+x^2)}dx\\\\\n",
    "&=\\int_2^\\infty\\frac{1}{\\pi(1+x^2)}dx=\\theta,\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "where $X$ is Cauchy. \n",
    "\n",
    "\n",
    "- Note that the method can be generalized to higher dimension:\n",
    "\n",
    "$$\n",
    "\\int_0^1\\int_0^1f(x_1,x_2)dx_1dx_2=E[f(X_1,X_2)],\n",
    "$$\n",
    "\n",
    "with $X_1,X_2\\stackrel{\\mathrm{i.i.d}}{\\sim}U(0,1)$.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Monte Carlo numerical algorithms\n",
    "\n",
    "\n",
    "**MC integration: Cauchy integral**\n",
    "\n",
    "\n",
    "- Suppose we want to compute the integral of a Cauchy(0,1) for $X \\geq 2$:\n",
    "\n",
    "\n",
    "<img src=\"Lect27_Fig1.png\" width=\"400\" height=\"400\">\n",
    "\n",
    "\n",
    "i.e. the following integral:\n",
    "\n",
    "$$\n",
    "\\theta=\\int_2^\\infty\\frac{1}{\\pi(1+x^2)}dx.\n",
    "$$\n",
    "\n",
    "\n",
    "- $\\theta$ can be computed analytically and is equal to $0.14758$.\n",
    "\n",
    "Here are other possibilities you can use to compute it in a Monte Carlo fashion.\n",
    "\n",
    "- *Solution 1*. $p\\sim\\mbox{Cauchy}$, $f(x)=I_{X>2}(x)$.\n",
    "\n",
    "$$\n",
    "\\begin{gather*}\n",
    "   \\hat{\\theta}_1=\\frac{\\#\\{x_i>2,1\\leq i\\leq n\\}}{n}=(\\mbox{proportion of $x_i>2$}).\\\\\n",
    "   \\mathit{var}(\\hat{\\theta}_1)=\\frac{\\theta(1-\\theta)}{n}\n",
    "   \\approx \\frac{0.14758 \\times (1-0.14758)}{n}=\\frac{0.126}{n}\n",
    "  \\end{gather*}\n",
    "$$  \n",
    "  since $n\\hat{\\theta}_1\\sim\\text{Bin}(n,\\theta)$.\n",
    "- *Solution 2*. $\\theta=P(X>2)=\\frac{1}{2}P(|X|>2)=\\frac{1}{2}E[I_{|X|>2}(x)]$.\n",
    "\n",
    "$$  \n",
    "  \\begin{gather*}\n",
    "  \\hat{\\theta}_2=\\frac{1}{2}\\frac{\\#\\{|x_i|>2,1\\leq i\\leq n\\}}{n}=\\frac{1}{2}(\\mbox{proportion of $|x_i|>2$})\\\\\n",
    "   \\mathit{var}(\\hat{\\theta}_2)=\\frac{n(2\\theta)(1-2\\theta)}{(2n)^2}\n",
    "   =\\frac{\\theta(1-\\theta)}{2n} \\approx\\frac{0.052}{n}\n",
    "  \\end{gather*}\n",
    "$$\n",
    "  \n",
    "(since $2n\\hat{\\theta}_2\\sim\\text{Bin}(n,2\\theta)$), an improvement with respect to the previous result.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Monte Carlo numerical algorithms\n",
    "\n",
    "\n",
    "**MC integration: Cauchy integral (cont'd)**\n",
    "\n",
    "\n",
    "- *Solution 3*. Another option is:\n",
    "\n",
    "$$1-2\\theta=\\int_{-2}^2p(x)dx=4\\int_0^2p(x)\\cdot\\frac{1}{2}dx$$\n",
    "\n",
    "$$=4\\int_0^2\\frac{1}{2}\\frac{1}{\\pi(1+y^2)}dy= 4E[f(Y)]$$,\n",
    "\n",
    "where $Y\\sim U(0,2)$. So, $\\theta=\\frac{1}{2}-2E[f(Y)]$.\n",
    "\n",
    "\n",
    "- Then:\n",
    "\n",
    "$$\n",
    "\\begin{gather*}\n",
    "  \\hat{\\theta}_3=\\frac{1}{2}-2\\cdot\\frac{1}{n}\\sum_{i=1}^nf(Y_i),\\;\n",
    "  Y_1,\\ldots,Y_n\\stackrel{\\mathrm{i.i.d.}}{\\sim}U(0,2)\\\\\n",
    "  \\mathit{var}(\\hat{\\theta}_3)=\\frac{4}{n}\\mathit{var}(f(Y_1))\\approx\\frac{0.028}{n},\n",
    "  \\end{gather*}\n",
    "$$\n",
    "\n",
    "a further improvement.\n",
    "\n",
    "<img src=\"Lect27_Fig2.png\" width=\"400\" height=\"400\">\n",
    "\n",
    "\n",
    "- *Solution 4*. Let $y=\\frac{2}{x}$ ($dx=-(2/y)^2dy$), then:\n",
    "\n",
    "$$\n",
    "\\begin{gather*}\n",
    "  \\theta_4=\\int_2^\\infty\\frac{1}{\\pi(1+x^2)}dx\n",
    " =\\int_0^{1}\\frac{2}{\\pi(4+y^2)}dy.\n",
    "   \\end{gather*}\n",
    "$$\n",
    "\n",
    "\n",
    "- This particular integral is in a form where it can be evaluated using crude Monte Carlo by sampling from the uniform distribution $U(0, 1)$. \n",
    "\n",
    "\n",
    "- Another improvement.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.1475836176504333, 1.3085563320472785e-10)\n"
     ]
    }
   ],
   "source": [
    "#Using quad:\n",
    "from scipy import integrate\n",
    "import numpy as np\n",
    "import math\n",
    "x2 = lambda x: 1/(math.pi *(1+ x**2))\n",
    "print(integrate.quad(x2, 2, np.inf))\n",
    "##########\n",
    "#Compare with empirical result (next slide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14549\n"
     ]
    }
   ],
   "source": [
    "#Solution 1:\n",
    "import numpy as np\n",
    "import math\n",
    "n = 100000\n",
    "counts = np.zeros(n)\n",
    "cauchy_values = np.random.standard_cauchy(n)\n",
    "for i in range(0,n):\n",
    "    if cauchy_values[i] > 2:\n",
    "        counts[i] = 1\n",
    "theta1 = np.sum(counts)/n\n",
    "print(theta1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14729\n"
     ]
    }
   ],
   "source": [
    "#Solution 2:\n",
    "import numpy as np\n",
    "import math\n",
    "n = 100000\n",
    "counts = np.zeros(n)\n",
    "cauchy_values = np.absolute(np.random.standard_cauchy(n))\n",
    "for i in range(0,n):\n",
    "    if cauchy_values[i] > 2:\n",
    "        counts[i] = 1\n",
    "theta2 = (1/2) * np.sum(counts)/n\n",
    "print(theta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1473974839682871\n"
     ]
    }
   ],
   "source": [
    "#Solution 3:\n",
    "import numpy as np\n",
    "import math\n",
    "n = 100000\n",
    "unif_values = np.random.uniform(0, 2, n)\n",
    "f_y = (1/(math.pi*(1+unif_values**2)))\n",
    "theta3 = 0.5 - 2 * (np.sum(f_y)/n)\n",
    "print(theta3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14758846136239862\n"
     ]
    }
   ],
   "source": [
    "#Solution 4:\n",
    "import numpy as np\n",
    "import math\n",
    "n = 100000\n",
    "unif_values = np.random.uniform(0, 1, n)\n",
    "f_y = (2/(math.pi*(4+unif_values**2)))\n",
    "theta4 = (np.sum(f_y)/n)\n",
    "print(theta4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Monte Carlo numerical algorithms\n",
    "\n",
    "\n",
    "**Importance Sampling (IS)**\n",
    "\n",
    "\n",
    "- With IS we want to estimate $\\theta=\\int f(x)p(x)dx=E_p[f(X)]$ (with $X$ having pdf $p$) through $\\hat{\\theta}_p=\\frac{1}{n}\\sum_{i=1}^n f(X_i)$ (with $(X_1,\\ldots,X_n) \\stackrel{\\mathrm{i.i.d.}}{\\sim}p$.  \n",
    " \n",
    "\n",
    "- The idea here is that instead of sampling directly from $p$, we sample from another pdf $g$. i.e., $(X_1,\\ldots,X_n)\\stackrel{\\mathrm{i.i.d.}}{\\sim}g$. \n",
    "\n",
    "\n",
    "\n",
    "- Our new estimate based on this $g$ will be: $\\hat{\\theta}_g=\\frac{1}{n}\\sum_{i=1}^n f(X_i)\\frac{p(X_i)}{g(X_i)}=\\frac{1}{n}\\sum_{i=1}^n \\psi(X_i)$. \n",
    "\n",
    "\n",
    "- Note that, by setting $\\psi(x)=f(x)\\frac{p(x)}{g(x)}$:\n",
    "$$\n",
    "\\begin{split}\n",
    "    \\theta &=\\int f(x)p(x)dx=\\int\\biggl[f(x)\\frac{p(x)}{g(x)}\\biggr]g(x)dx\\\\\n",
    "     &=\\int\\psi(x)g(x)dx\\\\\n",
    "     &=E[\\psi(X)],\\;\\;X\\sim g.\n",
    "   \\end{split}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Monte Carlo numerical algorithms\n",
    "\n",
    "\n",
    "**Importance Sampling (IS). Example**\n",
    "\n",
    "\n",
    "- Suppose we want to simulate the value of the integral:\n",
    "\n",
    "$$\n",
    "\\int^{\\infty}_{4.5}p(u)du,\n",
    "$$\n",
    "\n",
    "where $p(\\cdot)$ is the density of a r.v. $Z \\sim N(0,1)$. \n",
    "\n",
    "\n",
    "- We know from elementary statistics that this is rather a low value (you even cannot find the value in standardized normal tables!). \n",
    "\n",
    "\n",
    "- And in fact Python gives us:\n",
    "\n",
    "- ``In []: from scipy.stats import norm\n",
    "1-norm.cdf(4.5, loc=0, scale=1)``\n",
    "\n",
    "\n",
    "- ``Out []: 3.3976731247387093e-06``\n",
    "\n",
    "\n",
    "- This means that with crude Monte Carlo simulation we have to wait a lot before we get a hit, i.e. a simulated value greater than 4.5. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Monte Carlo numerical algorithms\n",
    "\n",
    "\n",
    "**Importance Sampling (IS). Example from Robert & Casella**.\n",
    "\n",
    "\n",
    "- We can use importance sampling in this case, simulating r.v. values from another density $g$. \n",
    "\n",
    "\n",
    "- Our $g$ will be an exponential truncated at $4.5$ which has density:\n",
    "\n",
    "$$\n",
    "g(y)=e^{-(y-4.5)}.\n",
    "$$ \n",
    "\n",
    "- Its graph compared with the $\\mathcal{E}\\text{xp}(1)$ is the following:\n",
    "\n",
    "\n",
    "<img src=\"MCInt6.png\" width=\"400\" height=\"400\">\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Monte Carlo numerical algorithms\n",
    "\n",
    "\n",
    "**Importance Sampling (IS). Example from Robert & Casella**\n",
    "\n",
    "\n",
    "- We draw $n$ values $y_i$ from $g$, then we calculate the ratios of the densities $\\frac{p}{g}$, obtaining the corresponding importance sampling estimator of the tail probability:\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\sum^{n}_{i=1}\\frac{p(Y^{(i)})}{g(Y^{(i)})}\n",
    "=\\frac{1}{n} \\sum^{n}_{i=1}\\frac{\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{1}{2}Y^{2}_{i}}}{e^{(-Y_{i}+4.5)}}=\\frac{1}{n}\\sum^{n}_{i=1}\\frac{e^{-(\\frac{Y^{2}_{i}}{2}+Y_{i}-4.5)}}{\\sqrt{2 \\pi}},\n",
    "$$\n",
    "\n",
    "where the $Y_i$'s are iid generations from $g$. \n",
    "\n",
    "\n",
    "- The corresponding Python code is the following producing a value remarkably close to the true value of $3.398\\times 10^{-6}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.39623384392559e-06"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "from scipy.stats import expon\n",
    "import numpy as np\n",
    "import math\n",
    "NSim = 10 ** 6\n",
    "y = np.random.exponential(1, NSim) + 4.5\n",
    "weit=norm.pdf(y)/expon.pdf(y-4.5)\n",
    "result = np.sum(weit)/NSim\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Monte Carlo numerical algorithms (cont'd)\n",
    "\n",
    "\n",
    "**The Expectation-Maximization (EM) algorithm** (Dempster et al., 1977; see Robert & Casella, pp. 152-163)\n",
    "\n",
    "\n",
    "- An important algorithm for detecting parameter estimation of *latent* variables. \n",
    "\n",
    "\n",
    "- Therefore, useful in missing data contexts, Bayesian networks, mixtures of distributions, even cluster detection. \n",
    "\n",
    "\n",
    "- Quite a heavy mathematical burden, so we will use a quick definition and a toy example.\n",
    "\n",
    "\n",
    "- Based on the concept of ML estimation (please revise it!).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Monte Carlo numerical algorithms (cont'd)\n",
    "\n",
    "\n",
    "**The Expectation-Maximization (EM) algorithm** (cont'd)\n",
    "\n",
    "\n",
    "- (EM) method is an iterative method for maximizing difficult likelihood functions in order to find MLE estimators. \n",
    "\n",
    "\n",
    "- Suppose we have a random sample $X=(X_1,\\cdots, X_n)$ iid from $f(x|\\theta)$. \n",
    "\n",
    "\n",
    "- We wish to find the ML estimator \n",
    "\n",
    "$$\\hat \\theta = \\arg \\max_{\\theta} \\prod_{i=1}^n f(x_i| \\theta) = \\arg \\max_{\\theta} \\sum_{i=1}^n \\log f(x_i|\\theta).$$ \n",
    "\n",
    "\n",
    "- Suppose we are in a situation where this optimization problem is difficult to solve. \n",
    "\n",
    "\n",
    "- We *augment* the data, i.e. we guess that an unobservable variable is governing this likelihood problem. \n",
    "\n",
    "\n",
    "- (That's why EM is suitable for missing data problems). \n",
    "\n",
    "\n",
    "- We denote these unobserved or missing data with $X^m$, such that we get the *complete data* $X^c = (X, X^m)$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Monte Carlo numerical algorithms (cont'd)\n",
    "\n",
    "\n",
    "**The Expectation-Maximization (EM) algorithm** (cont'd)\n",
    "\n",
    "\n",
    "-  The joint density of the complete data $X^c$ is:\n",
    "\n",
    "$$\n",
    "X^c=(X,X^m) \\sim f(x^c) = f(x,x^m).\n",
    "$$ \n",
    "\n",
    "\n",
    "- The conditional density for the missing data $X^m$ with respect to the observed data is:\n",
    "\n",
    "$$\n",
    "f(x^m|x,\\theta) = \\frac{f(x,x^m|\\theta)}{f(x|\\theta)}.\n",
    "$$ \n",
    "\n",
    "\n",
    "- Rearranging terms:\n",
    "\n",
    "$$\n",
    "f(x|\\theta) = \\frac{f(x,x^m|\\theta)}{f(x^m|x,\\theta)}.\n",
    "$$ \n",
    "\n",
    "\n",
    "- Taking logarithm, we get the log-likelihood:\n",
    "\n",
    "$$\n",
    "\\log f(X|\\theta) = \\log f(X^c|\\theta) - \\log f(X^m|X,\\theta).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Monte Carlo numerical algorithms (cont'd)\n",
    "\n",
    "\n",
    "**The Expectation-Maximization (EM) algorithm** (cont'd)\n",
    "\n",
    "\n",
    "-  Finally, taking expectation with respect to $f(x^m|x,\\theta_0)$ (considering a given value for $\\theta$, $\\theta_0)$ so that $X$ can be considered constant:\n",
    "\n",
    "\\begin{eqnarray*} \\mathbb{E} [\\log f(X|\\theta)] &=& \\mathbb{E} [\\log f(X^c|\\theta) |X,\\theta_0]\\\\\n",
    "&& - \\mathbb{E} [\\log f(X^m|X,\\theta)|X,\\theta_0].\n",
    "\\end{eqnarray*}\n",
    "\n",
    "\n",
    "- Let's denote the log-likelihood of the complete data (which is the focus of our algorithm) with the following expression:\n",
    "\n",
    "$$\n",
    "Q(\\theta|\\theta_0,x) = \\mathbb{E} [\\log f(X^c|\\theta)|X,\\theta_0].\n",
    "$$\n",
    "\n",
    "\n",
    "- The EM algorithm works going across these two steps until convergence is reached:\n",
    "    1. Expectation-step: compute $Q(\\theta|\\hat \\theta_{j-1},x)$;\n",
    "    2. Maximization-step: maximize $Q(\\theta|\\hat \\theta_{j-1},x)$ and take $$\\hat \\theta_j = \\arg \\max_{\\theta} Q(\\theta|\\hat \\theta_{j-1},x).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Monte Carlo numerical algorithms (cont'd)\n",
    "\n",
    "\n",
    "**The Expectation-Maximization (EM) algorithm** (cont'd)\n",
    "\n",
    "\n",
    "\n",
    "*A toy example* (or, *an example with toys* - presented in the original paper by Dempster et al., 1977).\n",
    "\n",
    "\n",
    "- We use a toy example (literally!) to explain how the EM algorithm works.\n",
    "\n",
    "\n",
    "- Imagine you ask $n$ kids to choose a toy out of 4 possible choices. \n",
    "\n",
    "\n",
    "- Let $Y = [Y_1, \\dots , Y_4]^T$ be the histogram of their $n$ choices, i.e. $Y_1$ is the number of kids that chose toy $1$, ..., $Y_4$ is the number of kids that chose toy $4$.\n",
    "\n",
    "\n",
    "- What is the r.v. that models this data?   A multinomial r.v. \n",
    "\n",
    "\n",
    "- Therefore, the histogram is \"distributed\" according to a multinomial distribution. \n",
    "\n",
    "\n",
    "- The multinomial has two sets of parameters: the number of trials $n$ and the probabilities $p_1, p_2, p_3, p_4$ of choosing toys $1,2,3,4$, respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Monte Carlo numerical algorithms (cont'd)\n",
    "\n",
    "\n",
    "**The Expectation-Maximization (EM) algorithm** (cont'd)\n",
    "\n",
    "\n",
    "\n",
    "*A toy example* (or, *an example with toys* - cont'd).\n",
    "\n",
    "\n",
    "- Therefore, the probability of seeing some particular histogram $y$ is:\n",
    "\n",
    "$$\n",
    "P(y|\\theta)=\\frac{n!}{y_1!y_2!y_3!y_4!}p_1^{y_1}p_2^{y_2}p_3^{y_3}p_4^{y_4}. (3)\n",
    "$$\n",
    "\n",
    "\n",
    "- For this example it is assumed that the vector of probabilities $p$ is parameterized by some hidden parameter $\\theta \\in (0,1)$ such that it can be written as:\n",
    "\n",
    "$$\n",
    "[p_1=\\frac{1}{2}+\\frac{1}{4}\\theta, p_2=\\frac{1}{4}(1-\\theta), p_3=\\frac{1}{4}(1-\\theta), p_4=\\frac{1}{4}(\\theta)]^T,\n",
    "$$\n",
    "so that $\\sum p_i=1$.\n",
    "\n",
    "\n",
    "- The estimation problem is to guess the value of $\\theta$ that maximizes the probability of the observed histogram.\n",
    "\n",
    "\n",
    "- For this simple example, one could directly maximize the log-likelihood $\\log P (y | \\theta)$, but here we will instead illustrate how to use the EM algorithm to find the MLE of $\\theta$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Monte Carlo numerical algorithms (cont'd)\n",
    "\n",
    "\n",
    "**The Expectation-Maximization (EM) algorithm** (cont'd)\n",
    "\n",
    "\n",
    "\n",
    "*A toy example* (or, *an example with toys* - cont'd).\n",
    "\n",
    "\n",
    "- With the parameterization above, we can write the probability in (3) as follows:\n",
    "\n",
    "$$\n",
    "P(y|\\theta)=\\frac{n!}{y_1!y_2!y_3!y_4!}[\\frac{1}{2}+\\frac{1}{4}\\theta]^{y_1}[\\frac{1}{4}(1-\\theta)]^{y_2}[\\frac{1}{4}(1-\\theta)]^{y_3}[\\frac{1}{4}(\\theta)]^{y_4}.\n",
    "$$\n",
    "\n",
    "\n",
    "- Now, in order to properly apply EM, we need to specify what the complete data $X$ is. \n",
    "\n",
    "\n",
    "- To that purpose, we define the complete data as $X=[X_1, \\dots , X_5]$ , with $X$ multinomial with number of trials $n$ and probability of each event:\n",
    "\n",
    "$$\n",
    "[p_1=\\frac{1}{2}, p_2=\\frac{1}{4}\\theta, p_3=\\frac{1}{4}(1-\\theta), p_4=\\frac{1}{4}(1-\\theta), p_5=\\frac{1}{4}(\\theta)]^T.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Monte Carlo numerical algorithms (cont'd)\n",
    "\n",
    "\n",
    "**The Expectation-Maximization (EM) algorithm** (cont'd)\n",
    "\n",
    "\n",
    "\n",
    "*A toy example* (or, *an example with toys* - cont'd).\n",
    "\n",
    "\n",
    "- By defining $X$ in this way, we can write the observed data $Y$ as:\n",
    "\n",
    "$$\n",
    "Y=T(X)=[X_1+X_2, X_3, X_4, X_5].\n",
    "$$\n",
    "\n",
    "\n",
    "- Therefore, the likelihood of a realization $x$ of the complete data is:\n",
    "\n",
    "$$\n",
    "P(x|\\theta)=\\frac{n!}{\\prod_{i=1}^5 x_i!}(\\frac{1}{2})^{x_1}(\\frac{\\theta}{4})^{x_2+x_5}(\\frac{1-\\theta}{4})^{x_3+x_4}.\n",
    "$$\n",
    "\n",
    "\n",
    "- For the EM algorithm, we should maximize the $Q$ function:\n",
    "\n",
    "$$\n",
    "\\theta^{(m+1)}= \\underset{\\theta \\in (0,1)}{\\arg\\max} Q(\\theta | \\theta^{(m)})=\\underset{\\theta \\in (0,1)}{\\arg\\max} E_{X|y, \\theta^{(m)}} [\\log p(X|\\theta)]. (4)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Monte Carlo numerical algorithms (cont'd)\n",
    "\n",
    "\n",
    "**The Expectation-Maximization (EM) algorithm** (cont'd)\n",
    "\n",
    "\n",
    "\n",
    "*A toy example* (or, *an example with toys* - cont'd).\n",
    "\n",
    "\n",
    "- To solve (4) for $\\theta$, we need the terms of $\\log p(X|\\theta)$ that depend on $\\theta$, because the other terms are irrelevant for maximizing the function over $\\theta$:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "&\\theta^{(m+1)}= \\underset{\\theta \\in (0,1)}{\\arg\\max} E_{X|y, \\theta^{(m)}} [(X_2+X_5)\\log \\theta + (X_3+X_4) \\log (1-\\theta)]\\\\\n",
    "&= \\underset{\\theta \\in (0,1)}{\\arg\\max} \\left\\{ \\log \\theta (E_{X|y, \\theta^{(m)}}[X_2] + E_{X|y, \\theta^{(m)}}[X_5]) + \\log(1-\\theta)(E_{X|y, \\theta^{(m)}}[X_3] + E_{X|y, \\theta^{(m)}}[X_4])\\right\\} (5)\n",
    "\\end{split}\n",
    "$$\n",
    "where we have considered only terms that depend on $\\theta$.\n",
    "\n",
    "\n",
    "- To solve (5) we need the conditional expectation of the complete data $X$ conditioned on already knowing the incomplete data $y$, which only leaves the uncertainty about $X_1$ and $X_2$.\n",
    "\n",
    "\n",
    "- But we know that $X_1+X_2=y_1$ , and therefore we can say that given $y_1$, the pair $X_1$, $X_2$ is binomially distributed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Monte Carlo numerical algorithms (cont'd)\n",
    "\n",
    "\n",
    "**The Expectation-Maximization (EM) algorithm** (cont'd)\n",
    "\n",
    "\n",
    "\n",
    "*A toy example* (or, *an example with toys* - cont'd).\n",
    "\n",
    "\n",
    "- Exploiting the last point in the previous slide, we end up with this conditional distribution to be plugged in (5) to solve the problem:\n",
    "\n",
    "$$\n",
    "E_{X|y, \\theta[X]}=[\\frac{2}{2+ \\theta}y_1, \\frac{\\theta}{2+ \\theta}y_1, y_2, y_3, y_4].\n",
    "$$\n",
    "\n",
    "\n",
    "- Therefore (4) becomes:\n",
    "$$\n",
    "\\begin{split}\n",
    "&\\theta^{(m+1)}=\\\\\n",
    "&= \\underset{\\theta \\in (0,1)}{\\arg\\max} (\\log \\theta(\\frac{\\theta^{(m)}y_1}{2+ \\theta^{(m)}}+y_4) + \\log (1-\\theta)(y_2+y_3))\\\\\n",
    "&=\\frac{\\frac{\\theta^{(m)}}{2+\\theta^{(m)}}y_1+y_4}{\\frac{\\theta^{(m)}}{2+\\theta^{(m)}}y_1+y_2+y_3+y_4}.\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suggested references and reading\n",
    "\n",
    "\n",
    "\n",
    "- Brewer, B.J., Introduction to Bayesian Statistics. Course notes. University of Auckland.\n",
    "\n",
    "\n",
    "- De Finetti, B (1931). Funzione caratteristica di un fenomeno aleatorio. Accademia dei Lincei, Roma.\n",
    "\n",
    "\n",
    "- Dempster, A.P., Laird, N. M., Rubin, D. B. (1977). Maximum Likelihood from Incomplete Data via the EM Algorithm. Journal of the Royal Statistical Society, Series B. 39(1): 1-38.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example with a mixture of Normals using the *sklearn GaussianMixture class* which implements the EM algorithm for fitting a mixture of normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2],\n",
       "       [ 1,  4],\n",
       "       [ 1,  0],\n",
       "       [10,  2],\n",
       "       [10,  4],\n",
       "       [10,  0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0]]) #List of n_features-dimensional data points. \n",
    "#Each row corresponds to a single data point.\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.  2.]\n",
      " [ 1.  2.]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m gm \u001b[38;5;241m=\u001b[39m GaussianMixture(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(gm\u001b[38;5;241m.\u001b[39mmeans_)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mplot\u001b[49m(gm)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#two 2-sized means, centres of the components\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Predict the labels for the data samples in X using trained model:\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(gm\u001b[38;5;241m.\u001b[39mpredict([[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m3\u001b[39m]]))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plot' is not defined"
     ]
    }
   ],
   "source": [
    "gm = GaussianMixture(n_components=2, random_state=0).fit(X)\n",
    "print(gm.means_)\n",
    "#two 2-sized means, centres of the components\n",
    "# Predict the labels for the data samples in X using trained model:\n",
    "print(gm.predict([[0, 0], [12, 3]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "div.cell { /* Tunes the space between cells */\n",
       "margin-top:1em;\n",
       "margin-bottom:1em;\n",
       "}\n",
       "\n",
       "div.text_cell_render h1 { /* Main titles bigger, centered */\n",
       "font-size: 2.2em;\n",
       "line-height:1.4em;\n",
       "text-align:center;\n",
       "}\n",
       "\n",
       "div.text_cell_render h2 { /*  Parts names nearer from text */\n",
       "margin-bottom: -0.4em;\n",
       "}\n",
       "\n",
       "\n",
       "div.text_cell_render { /* Customize text cells */\n",
       "font-family: 'Times New Roman';\n",
       "font-size:1.5em;\n",
       "line-height:1.4em;\n",
       "padding-left:3em;\n",
       "padding-right:3em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To run slideshow type jupyter nbconvert /Users/giancarlomanzi/Documents/Box Sync BackUp PC Lavoro 24062015/documenti/Didattica/Corso Advanced Multivariate Statistics/Notebooks and Lectures/Lecture 2 Bootstrap 2/Bootstrap2.ipynb --to slides --post serve\n",
    "# from terminal\n",
    "#/Users/giancarlomanzi/Documents/Box Sync BackUp PC Lavoro 24062015/documenti/Didattica/Corso Advanced Multivariate Statistics/Notebooks and Lectures/Lecture 2 Bootstrap 2/Bootstrap2.ipynb\n",
    "\n",
    "#This is to let you have larger fonts...\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    "\n",
    "div.cell { /* Tunes the space between cells */\n",
    "margin-top:1em;\n",
    "margin-bottom:1em;\n",
    "}\n",
    "\n",
    "div.text_cell_render h1 { /* Main titles bigger, centered */\n",
    "font-size: 2.2em;\n",
    "line-height:1.4em;\n",
    "text-align:center;\n",
    "}\n",
    "\n",
    "div.text_cell_render h2 { /*  Parts names nearer from text */\n",
    "margin-bottom: -0.4em;\n",
    "}\n",
    "\n",
    "\n",
    "div.text_cell_render { /* Customize text cells */\n",
    "font-family: 'Times New Roman';\n",
    "font-size:1.5em;\n",
    "line-height:1.4em;\n",
    "padding-left:3em;\n",
    "padding-right:3em;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
