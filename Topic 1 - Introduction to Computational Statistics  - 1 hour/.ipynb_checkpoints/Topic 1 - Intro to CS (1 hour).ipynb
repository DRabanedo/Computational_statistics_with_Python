{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2f32ec5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <p style=\"text-align: center;\">Computational Statistics with Python</p>\n",
    "\n",
    "\n",
    "## <p style=\"text-align: center;\">Topic 1: Introduction to Computational Statistics</p>\n",
    "\n",
    "## <p style=\"text-align: center;\">Expected lecture time: 1 hour</p>\n",
    "\n",
    "\n",
    "<p style=\"text-align: center;\">Giancarlo Manzi</p>\n",
    "<p style=\"text-align: center;\">Department of Economics, Management and Quantitative Methods</p>\n",
    "<p style=\"text-align: center;\">University of Milan, Milan, Italy</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1a6c8b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <p style=\"text-align: center;\">Course level and shape</p>\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "- Course on **statistical simulation** when theory might fail.\n",
    "\n",
    "\n",
    "- Special focus on **Monte Carlo methods** for random value and variable generation.\n",
    "\n",
    "\n",
    "- Very few rigorous proofs and mathematical details, a lot of intuition and **see-how it-happens applications**.\n",
    "\n",
    "\n",
    "- Special focus on **Python**.\n",
    "\n",
    "\n",
    "- This is a huge and continuously developing field; therefore not enough time to see everything; need to **choose selected topics**.\n",
    "\n",
    "\n",
    "- Basic **statistical knowledge** required. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456df43f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <p style=\"text-align: center;\">Course Goals</p>\n",
    "    \n",
    "- **Goal 1**: To introduce the Python language and the Anaconda platform (mainly Jupyter).\n",
    "\n",
    "\n",
    "- **Goal 2**: To present a few Monte-Carlo methods through Python,  namely:\n",
    "    - Resampling methods.\n",
    "    - Random numbers and random variable generation.\n",
    "    - Basic MCMC methods.\n",
    "    - Other MC methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d56c8a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <p style=\"text-align: center;\">Details</p>\n",
    "\n",
    "\n",
    "- **Basic coding in Python** (mainly bootstrap, but also jackknife, modified bootstrap)\n",
    "\n",
    "\n",
    "- **Pseudo-Random number generators** (Examples: linear congruential generators, multiply with carry generators, lagged Fibonacci generators, etc.).\n",
    "\n",
    "\n",
    "- **Random variable generation** (Examples: the inverse transform method; the accept-reject method; the Box-Muller method).\n",
    "\n",
    "\n",
    "- **Monte Carlo numerical methods** (the Newton-Raphson algorithm, Monte Carlo integration, the EM algorithm, importance sampling, etc.).\n",
    "\n",
    "\n",
    "- Review of **basic Bayesian analysis**.\n",
    "\n",
    "\n",
    "- **Mixture models** (just an outline).\n",
    "\n",
    "\n",
    "- **Monte Carlo Markov Chain basics**: the **Metropolis-Hastings algorithm**.\n",
    "\n",
    "\n",
    "- **MCMC basics**: the **Gibbs sampler**.\n",
    "\n",
    "\n",
    "- **Hierarchical Bayesian models**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17588c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Textbooks and other material\n",
    "\n",
    "- Efron, B., Tibshirani, R. J. (1993). *An Introduction to the Bootstrap*. New York: CRC Press.\n",
    "\n",
    "\n",
    "- Kenett, R., Zacks, S., Gedeck, P. (2022). Modern Statistics: A Computer Based Approach with Python. *Forthcoming*. \n",
    "\n",
    "\n",
    "-  Guttag, J. V. (2020). Introduction to Computation and Programming Using Python.\n",
    "\n",
    "\n",
    "- Lunn, D., Jackson, C., Best, N., Thomas, A., Spiegelhalter, D. (2012). *The BUGS Book*. New York: CRC Press.\n",
    "\n",
    "\n",
    "- Martin, O. A., Kumar, R., Lao, J. (2022). *Bayesian Modeling and Computation in Python*. CRC Press.\n",
    "\n",
    "\n",
    "- Notebooks, R scripts, other material, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ea3ea9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is computational statistics?\n",
    "\n",
    "- Definition 1 (Wegman, 1988): *CS is a collection of techniques that have a strong focus on the exploitation of computing in the creation of new statistical methodology*.\n",
    "\n",
    "\n",
    "- Definition 2 (Efron and Tibshirani, 1991): *CS is a collection of computer-intensive statistical methods*.\n",
    "\n",
    "\n",
    "- Definition 3 (Rao, 1993): *CS refers to the trend in modern statistics of basic methodology supported by the state-of-the-art computational and graphical facilities*.\n",
    "\n",
    "\n",
    "- Definition 4 (Gentle, 2005): *CS is a class of statistical methods characterized by computational methods*.\n",
    "\n",
    "\n",
    "- My personal definition - part I (also including simulation and Monte Carlo techniques): **CS is a set of tools to \"survive\" when traditional statistical theory doesn't help**.\n",
    "\n",
    "\n",
    "- My personal definition - part II (also including simulation and Monte Carlo techniques): **CS is a set of tools to \"solve\" challenging theoretical problems**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658d8e98",
   "metadata": {},
   "source": [
    "## A computational statistics word cloud\n",
    "\n",
    "<img src=\"wordcloud.png\" width=\"800\" height=\"400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed846358",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## An interesting comparison\n",
    "\n",
    "| Feature | Traditional statistics | Computational statistics |\n",
    "| --- | --- | --- |\n",
    "| Sample size | Small to moderate | Large to very large |\n",
    "| Sample selection | Independent Identically Distributed | Nonhomogeneous |\n",
    "| Number of random variables | One or a few | High number |\n",
    "| Type of computation | Close to manual | Intensive |\n",
    "| Mathematical burden | Theoretically tractable | Numerically tractable |\n",
    "| Type of research question | Precise | Imprecise |\n",
    "| Assumptions | Strong | Weak or nonexistent |\n",
    "| Relationships | Generally linear or linearizable | Generally nonlinear |\n",
    "| Population distribution | Known | Unknown |\n",
    "| Errors | Normal | Distribution free |\n",
    "| Type of inference | Mainly frequentist | Mainly Bayesian |\n",
    "| Type of used algorithm | In closed forms | Mainly iterative |\n",
    "| Statistical properties | Optimal | Robust |\n",
    "\n",
    "(Source: adapted from Wegman, 1988)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6e5abc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## An example of problems with integration\n",
    "   \n",
    "- An integral like the following:\n",
    "\n",
    "$$\n",
    "\\int_0^1 \\frac{1}{2\\sqrt{2 \\pi}} x^4(1-x)^3 e^{-\\frac{1}{8}(log(\\frac{x}{1-x})-4)^2}dx\n",
    "$$\n",
    "\n",
    "might be considered *analytically intractable*, meaning that either we cannot solve it or it has has no closed-form solution.\n",
    "\n",
    "\n",
    "- This was the problem pioneer Bayesians had until the very end of last century because often the posterior densities they obtained from Bayesian analysis were intractable.\n",
    "\n",
    "\n",
    "- Therefore we have to *approximate* it via simulation or numerical solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cd03ae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Another example of an approximate solution\n",
    "   \n",
    "- You probably remember that in *R* there is the constant *pi* giving the value 3.141593.\n",
    "\n",
    "\n",
    "- $\\pi$ is a real number and therefore *R* gives us an approximation of this real number with a lot of decimals (according to the floating point number memory).\n",
    "\n",
    "\n",
    "- How is this approximation done?\n",
    "\n",
    "- This is a numerical approximation exploiting geometry features and the uniform distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4074992f",
   "metadata": {},
   "source": [
    "## A broad (and incomplete) vision of computational statistics and statistical computing\n",
    "\n",
    "<img src=\"Taxonomy.png\" width=\"800\" height=\"400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba35b08",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A few step- by-step example of an approx solution used in computational Statistics\n",
    "   \n",
    "- The maximum likelihood estimation (MLE) method is the most popular method to obtain estimators for parameters in statistics.\n",
    "\n",
    "\n",
    "- It implies the computation of derivatives of sometimes too complicated functions to be solved analytically.\n",
    "\n",
    "\n",
    "- Closed form expressions for the MLEs might generally not exist for most of the models, especially in the regression context.\n",
    "\n",
    "\n",
    "- For example to find the estimators in the logit model one should use the MLE method.\n",
    "\n",
    "\n",
    "- Sometimes to solve for MLE is difficult and one has to rely on numerical approximation.\n",
    "\n",
    "\n",
    "- Then, some numerical approximation algorithms are used, such as the Newton-Raphson algorithm, the Fisher scoring method, the Iterative least square method.\n",
    "\n",
    "\n",
    "- Let's see here how the Newton-Raphson works in general and with a binomial example. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5396134a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Newton-Rapshon algorithm\n",
    "\n",
    "\n",
    "- The basic idea behind the NR algorithm is the following. \n",
    "\n",
    "    - (i) Construct a quadratic approximation to the function of interest around some initial parameter value (hopefully close to the MLE). \n",
    "    \n",
    "    - (ii) Adjust the parameter value to that which maximizes the quadratic approximation. \n",
    "    \n",
    "    - This two steps are iterated until the parameter values stabilize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea40ad88",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finding the maximum of a function....\n",
    "\n",
    " **Taylor series approximation**.\n",
    "\n",
    "\n",
    "- Let's start from the basic Taylor's theorem:\n",
    "*Suppose $f$ is $k+1$ times differentiable on an interval $I$. For any points $x$ and $x+h$ in $I$ there exists a point $w$ between $x$ and $x + h$ s.t.:\n",
    "$$\n",
    "f(x+h) = f(x) + f^{\\prime}(x)h + \\frac{1}{2}f^{\\prime \\prime} (x) h ^2 + \\dots + \\frac{1}{k!}f^{\\left[k\\right]}(x) h^{k} + \\frac{1}{(k+1)!}f^{\\left[k+1\\right]}(w)h^{k+1}.\n",
    "$$\n",
    "\n",
    "- It can be shown that as $h$ goes to $0$, the higher order terms in equation 1 go to $0$ much faster than $h$ goes to $0$. In this way we obtain (for small values of $h$) the *first order Taylor approximation of $f$ at $x$*:\n",
    "$$\n",
    "f(x + h) \\approx f(x) + f^{\\prime}(x)h.\n",
    "$$\n",
    "\n",
    "- Similarly, we obtain the *second order Taylor approximation of $f$ at $x$* in this way:\n",
    "$$\n",
    "f(x + h) \\approx f(x) + f^{\\prime}(x)h + \\frac{1}{2}f^{\\prime \\prime}(x) h^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe05475",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finding the maximum of a function.... (cont'd)\n",
    "\n",
    " **Taylor series approximation (cont'd)**.\n",
    "\n",
    "\n",
    "- We can axpress the first order Taylor's approximation in terms of a linear function in $h$:\n",
    ":\n",
    "$$\n",
    "f(x + h) \\approx a + bh,\n",
    "$$\n",
    "\n",
    "where $a=f(x)$ and $b = f^{\\prime}(x)$.\n",
    "\n",
    "- Similarly, we can express the second order Taylor's approximation in terms of a polynomial in $h$:\n",
    "$$\n",
    "f(x + h) \\approx a + bh + \\frac{1}{2} c h^2,\n",
    "$$\n",
    "where $a = f(x)$, $ b = f^{\\prime}(x)$ and $c = f^{\\prime \\prime}(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb76908",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finding the maximum of a function.... (cont'd)\n",
    "\n",
    " **Taylor series approximation (cont'd): finding a maximum of a second order polynomial**.\n",
    "\n",
    "\n",
    "- Suppose we want to find the value of $x$ that maximizes:\n",
    "$$\n",
    "f(x) =  a + bx + cx^2.\n",
    "$$\n",
    "\n",
    "- Let's compute the first derivative of $f$:\n",
    "$$\n",
    "f^{\\prime}(x) = b + 2cx,\n",
    "$$\n",
    "\n",
    "- Solving for $f^{\\prime} (\\hat{x})=0$ we obtain the first condition for a maximum:\n",
    "$$\n",
    "b + 2c\\hat{x} = 0,\n",
    "$$\n",
    "that is, $\\hat{x} = -\\frac{b}{2c}$.\n",
    "\n",
    "- Solving for $f^{\\prime \\prime} (x) < 0$ we obtain the second condition for a maximum:\n",
    "$$\n",
    "2c < 0.\n",
    "$$\n",
    "\n",
    "- This means that in $f(-\\frac{b}{2c})$ we have a maximum if $c < 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d1451a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The NR algorithm to finding maxima of functions\n",
    "\n",
    "\n",
    "- Suppose we want to find the value of x that maximizes some twice continuously differentiable function $f(x)$.\n",
    "\n",
    "- Since\n",
    "$$\n",
    "f(x + h) \\approx a + bh + \\frac{1}{2} c h^2,\n",
    "$$\n",
    "with $a = f(x)$, $ b = f^{\\prime}(x)$ and $c = f^{\\prime \\prime}(x)$, we have:\n",
    "$$\n",
    "f^{\\prime}(x+h) \\approx b + ch.\n",
    "$$\n",
    "\n",
    "- The first order condition for the value of $h$ (denoted $\\hat{h}$) that maximizes $f(x + h)$ is\n",
    "$$\n",
    "b + c\\hat{h}= 0,\n",
    "$$\n",
    "i.e. $\\hat{h} = - \\frac{b}{c}$.\n",
    "\n",
    "- Therefore, the value that maximizes the second order Taylor approximation to $f$ at $x$ is:\n",
    "$$\n",
    "x + \\hat{h} = x - \\frac{b}{c} = x - \\frac{f^{\\prime}(x)}{f^{\\prime \\prime}(x)}.\n",
    "$$\n",
    "and this is the basis for the NR algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f53ce03",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The NR algorithm to finding maxima of functions (cont'd)\n",
    "\n",
    "\n",
    "- With this in mind we can specify the Newton Raphson algorithm for 1 dimensional function optimization.\n",
    "\n",
    "**NR ALGORITHM: find the value $\\hat{x}$  of $x$ that maximizes $f(x)$**\n",
    "\n",
    "- [Input]: a function $f$, an initial value $x_0$, a tolerance level $tol$, an iterative value $i$ set to $0$.\n",
    "\n",
    "- While $|f^{\\prime}(x_i)|> tol$ do:\n",
    "    - $i = i +1$;\n",
    "    - $x_i = x_{i-1} - \\frac{f^{\\prime}(x_{i-1})}{f^{\\prime \\prime}(x_{i-1})}$;\n",
    "    - $\\hat{x} = x_i$.\n",
    "    \n",
    "- Return ($\\hat{x}$)\n",
    "\n",
    "\n",
    "\n",
    "**A BIG WARNING: the NR algorithm doesn’t check the second order conditions necessary for $\\hat{x}$ to be a maximizer. This means that if you give the algorithm a bad starting value for $x_0$ you end up with a min rather than a max**!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1246987",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The NR algorithm to finding maxima of functions: a Binomial example\n",
    "\n",
    "\n",
    "- Recall the density of a binomial random variable $y$ with parameter $n$ and $\\pi$:\n",
    "$$\n",
    "f(y| n, \\pi ) = \\binom{n}{y} \\pi^y (1-\\pi)^{n-y},\n",
    "$$\n",
    "for $y =0,1,2, \\dots , n$.\n",
    "\n",
    "- The log-likelihood function with respect to $\\pi$ (and without considering the binomial coefficient in which there is not a $\\pi$ term, so not contributing to the derivatives) is:\n",
    "$$\n",
    "\\ell (\\pi | n, y) = y \\ln (\\pi) + (n-y) \\ln (1-\\pi).\n",
    "$$\n",
    "\n",
    "- Let's compute the first derivative with respect to $\\pi$:\n",
    "$$\n",
    "\\ell^{\\prime} (\\pi| n,y) = \\frac{y}{\\pi}+ (n-y) \\frac{-1}{1-\\pi}.\n",
    "$$\n",
    "\n",
    "- Let's compute the second derivative with respect to $\\pi$:\n",
    "$$\n",
    "\\ell^{\\prime \\prime} (\\pi| n,y) = -\\frac{y}{\\pi^2} -  \\frac{n-y}{(1-\\pi)^2}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e176dc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The NR algorithm to finding maxima of functions: a Binomial example (cont'd)\n",
    "\n",
    "\n",
    "- Analytically, we know that the MLE is $\\hat{\\pi} = \\frac{y}{n}$.\n",
    "\n",
    "- Let's see if we the NR we obtain a good approximation in the case of $y=2$ and $n = 5$ for which then $\\hat{\\pi} = 0.4$.\n",
    "\n",
    "- We begin by setting a tolerance level. \n",
    "\n",
    "- In this case, let’s set it to $0.01$ (In practice you probably want something closer to 0.00001). \n",
    "\n",
    "- Next we make an initial guess (denoted $π_0$) as to the MLE. \n",
    "\n",
    "- Suppose $π_0 = 0.55$. Then $\\ell^{\\prime} (π_0|y) \\approx −3.03$ which is larger in absolute value than our tolerance of 0.01. Thus we set:\n",
    "$$\n",
    "\\pi_1 = \\pi_0 - \\frac{\\ell^{\\prime}(\\pi_0 | t)}{\\ell^{\\prime \\prime}(\\pi_0 | y)} \\approx 0. 40857.\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56aef6e",
   "metadata": {},
   "source": [
    "## The NR algorithm to finding maxima of functions: a Binomial example (cont'd)\n",
    "\n",
    "- Now we calculate $\\ell^{\\prime}(\\pi_1|y) \\approx −0.1774$ which is still larger in absolute value than our tolerance of $0.01$. Thus we set:\n",
    "$$\n",
    "\\pi_2 = \\pi_1 - \\frac{\\ell^{\\prime}(\\pi_1 | y)}{\\ell^{\\prime \\prime}(\\pi_1 | y)} \\approx 0.39994.\n",
    "$$\n",
    "\n",
    "- Now $\\ell^{\\prime}(\\pi_2|y) \\approx 0.0012 $ which is smaller in absolute value than our tolerance of 0.01 so we can stop and we are very close to the analytical solution after only two iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff986d9",
   "metadata": {},
   "source": [
    "## The Fisher scoring method\n",
    "\n",
    "- In its simplest form (for example with one parameter only) the Fisher scoring method uses the expected Fisher information instead of the second derivative in the NR algorithm.\n",
    "\n",
    "- So if we have a parameter $\\pi$, for a given step in the algorithm:\n",
    "$$\n",
    "    \\pi_i = \\pi_{i-1} + \\frac{\\ell^{\\prime}(\\pi_{i-1} | y)}{E\\left[ \\ell^{\\prime \\prime} (\\pi_{i-1}|y)\\right]}\n",
    "$$\n",
    "\n",
    "and the algorithm go ahead as the NR until the stopping rule (i.e. considering the tolerance value) is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833ea988",
   "metadata": {},
   "source": [
    "## The Iterative Least Squares algorithm\n",
    "\n",
    "- Apart from some details, the Fisher scoring in the context of model fitting, particularly in the regression context, is equivalent to the Iterative Least Squares algorithm (this was proved by McCullagh and Nelder (1989)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4a322a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>x\n",
       "\n",
       "div.cell { /* Tunes the space between cells */\n",
       "margin-top:1em;\n",
       "margin-bottom:1em;\n",
       "}\n",
       "\n",
       "div.text_cell_render h1 { /* Main titles bigger, centered */\n",
       "font-size: 2.2em;\n",
       "line-height:1.4em;\n",
       "text-align:center;\n",
       "}\n",
       "\n",
       "div.text_cell_render h2 { /*  Parts names nearer from text */\n",
       "margin-bottom: -0.4em;\n",
       "}\n",
       "\n",
       "\n",
       "div.text_cell_render { /* Customize text cells */\n",
       "font-family: 'Times New Roman';\n",
       "font-size:1.5em;\n",
       "line-height:1.4em;\n",
       "padding-left:3em;\n",
       "padding-right:3em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is to let you have larger fonts...\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>x\n",
    "\n",
    "div.cell { /* Tunes the space between cells */\n",
    "margin-top:1em;\n",
    "margin-bottom:1em;\n",
    "}\n",
    "\n",
    "div.text_cell_render h1 { /* Main titles bigger, centered */\n",
    "font-size: 2.2em;\n",
    "line-height:1.4em;\n",
    "text-align:center;\n",
    "}\n",
    "\n",
    "div.text_cell_render h2 { /*  Parts names nearer from text */\n",
    "margin-bottom: -0.4em;\n",
    "}\n",
    "\n",
    "\n",
    "div.text_cell_render { /* Customize text cells */\n",
    "font-family: 'Times New Roman';\n",
    "font-size:1.5em;\n",
    "line-height:1.4em;\n",
    "padding-left:3em;\n",
    "padding-right:3em;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec4d2ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
